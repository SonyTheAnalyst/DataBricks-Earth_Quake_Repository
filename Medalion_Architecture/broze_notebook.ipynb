{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3394f33-338e-4582-b4dc-e3afeaf2e279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define the stages of the Medallion Architecture data pipeline\n",
    "tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "# 2. Automatically generate the storage URLs for each tier.\n",
    "# This creates a dictionary where each tier is a key, and the Azure storage path is the value.\n",
    "# The 'f' before the string allows us to plug the {tier} variable directly into the URL.\n",
    "adls_paths = {tier: f\"abfss://{tier}@earthquakestorageaccount.dfs.core.windows.net/\" for tier in tiers}\n",
    "adls_paths\n",
    "\n",
    "\n",
    "# 3. Pull the specific URLs out of our dictionary and save them to easy-to-use variables.\n",
    "# This is like taking a long address from a directory and writing it on a sticky note.\n",
    "bronze_adls = adls_paths[\"bronze\"]\n",
    "silver_adls = adls_paths[\"silver\"]\n",
    "gold_adls = adls_paths[\"gold\"] \n",
    "\n",
    "# 4. Use Databricks Utilities (dbutils) to list the files in each folder.\n",
    "# This confirms that the connection to Azure is working and shows you what data is available.\n",
    "dbutils.fs.ls(bronze_adls)\n",
    "dbutils.fs.ls(silver_adls)\n",
    "dbutils.fs.ls(gold_adls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66703c6b-981f-4fc6-8653-3ded04201097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86ce887-2e34-4faf-b6fe-4322e8a5e17f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. 'requests' is used to make HTTP calls. \n",
    "# It's essentially the \"web browser\" for your code so it can talk to the Earthquake API.\n",
    "import requests\n",
    "# 2. 'json' helps Python read and write the JSON data format.\n",
    "# Since the Earthquake API sends data as JSON, we need this to translate it into a Python dictionary.\n",
    "import json\n",
    "# 3. 'datetime' allows us to work with dates and times.\n",
    "# 'date' handles specific days, and 'timedelta' allows us to do \"date math\" (like Today - 1 day).\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e6c198-8591-47ee-a3f5-690cf9b8d9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_date = date.today() - timedelta(1)\n",
    "end_date = date.today()\n",
    "start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6711f5d6-67c9-4ed7-8e5b-3a50eaeb9649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Build the specific \"Order Form\" for the USGS website.\n",
    "# We use an f-string to plug our dates and the 'geojson' format into the address.\n",
    "url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}\"\n",
    "\n",
    "try:\n",
    "    # 2. Go to the website and grab the data (the GET request).\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 3. Safety Check: If the website is down or the URL is wrong, \n",
    "    # this will stop the code and tell us exactly what went wrong (e.g., 404 Not Found).\n",
    "    response.raise_for_status() \n",
    "\n",
    "    # 4. Extract the 'features'. \n",
    "    # The API sends back metadata (extra info) we don't need; '.get('features')' \n",
    "    # grabs only the list of actual earthquake events.\n",
    "    data = response.json().get('features', [])\n",
    "\n",
    "    if not data:\n",
    "        # If there were zero earthquakes yesterday, we don't want to save an empty file.\n",
    "        print(\"No data returned for the specified date range.\")\n",
    "    else:\n",
    "        # 5. Define the destination. \n",
    "        # We name the file using the date so we don't overwrite yesterday's data.\n",
    "        file_path = f\"{bronze_adls}/{start_date}_earthquake_data.json\"\n",
    "\n",
    "        # 6. Make it \"Human Readable\".\n",
    "        # 'indent=4' adds spaces and line breaks so the file isn't just one long, impossible-to-read line.\n",
    "        json_data = json.dumps(data, indent=4)\n",
    "        \n",
    "        # 7. Write to the Cloud.\n",
    "        # 'dbutils.fs.put' sends the data to your 'Bronze' storage in Azure.\n",
    "        dbutils.fs.put(file_path, json_data, overwrite=True)\n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # 8. Error Handling: This catches internet connection issues or API timeouts.\n",
    "    print(f\"Error fetching data from API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9360a523-6d89-4ab3-a3d7-72ff22097ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352f31d9-721f-487c-bf11-f0dcf8f434a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Bundle all our important variables into a single \"packet\" (a dictionary).\n",
    "# Instead of passing items one by one, we put them all in this 'output_data' container.\n",
    "\n",
    "output_data = {\n",
    "    \"start_date\": start_date.isoformat(),\n",
    "    \"end_date\": end_date.isoformat(),\n",
    "    \"bronze_adls\": bronze_adls,\n",
    "    \"silver_adls\": silver_adls,\n",
    "    \"gold_adls\": gold_adls\n",
    "}\n",
    "\n",
    "#serialized the json format\n",
    "\"\"\"\n",
    "\n",
    "# Serialize the dictionary to a JSON string\n",
    "output_json = json.dumps(output_data)\n",
    "\n",
    "# Log the serialized JSON for debugging\n",
    "print(f\"Serialized JSON: {output_json}\")\n",
    "\n",
    "# Return the JSON string\n",
    "dbutils.notebook.exit(output_json)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 2. Use the Databricks \"Task Values\" utility to broadcast this information.\n",
    "# This saves the 'output_data' under the name \"bronze_output\" so that \n",
    "# subsequent tasks in the Databricks Job can read and use these values.\n",
    "\n",
    "\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_output\", value=output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4cf1dd-aebb-4c9d-b172-de404847ff65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bed87c4-5275-4a68-b856-8c681d398354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242bbff6-a965-4836-b52c-f6e73f1a226a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ea0e64-72c8-4b39-83a8-1cc62d72923b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "broze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

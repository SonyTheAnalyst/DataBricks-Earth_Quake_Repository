{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a505a4f-0b25-4ede-a068-c4d91d087cd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Reach out to the \"Bulletin Board\" (Task Values) from the previous task (bronze & silver).\n",
    "# We tell it: \"Go find the task named 'bronze' and give me the value labeled 'bronze_output'.\"\n",
    "bronze_output = dbutils.jobs.taskValues.get(taskKey=\"Bronze\", key=\"bronze_output\") #TASKEY IS THE NAME OF THE TASK IN THE JOB RUN EDITOR\n",
    "silver_data = dbutils.jobs.taskValues.get(taskKey= \"Silver\", key= \"silver_output\")  \n",
    "\n",
    "# 2. Extract the variables from that \"package\".\n",
    "\n",
    "# The .get(\"key\", \"\") is a safety feature: if the key is missing, it returns an empty string instead of crashing.\n",
    "start_date = bronze_output.get(\"start_date\", \"\")\n",
    "end_date = bronze_output.get(\"end_date\", \"\")\n",
    "gold_adls = bronze_output.get(\"gold_adls\", \"\")\n",
    "\n",
    "# 3. Print the results to verify we have the right \"keys\" to the storage account.\n",
    "\n",
    "print(f\"Start Date: {start_date}, Gold ADLS: {gold_adls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07c873b-e409-4d9f-b7de-fe404bf125b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Define the stages of the Medallion Architecture data pipeline\n",
    "tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "# 2. Automatically generate the storage URLs for each tier.\n",
    "# This creates a dictionary where each tier is a key, and the Azure storage path is the value.\n",
    "# The 'f' before the string allows us to plug the {tier} variable directly into the URL.\n",
    "adls_paths = {tier: f\"abfss://{tier}@earthquakestorageaccount.dfs.core.windows.net/\" for tier in tiers}\n",
    "\n",
    "\n",
    "# 3. Pull the specific URLs out of our dictionary and save them to easy-to-use variables.\n",
    "# This is like taking a long address from a directory and writing it on a sticky note.\n",
    "bronze_adls = adls_paths[\"bronze\"]\n",
    "silver_adls = adls_paths[\"silver\"]\n",
    "gold_adls = adls_paths[\"gold\"] \n",
    "\n",
    "silver_data = f\"{silver_adls}earthquake_events_silver/\"\n",
    "\n",
    "\n",
    "# 4. Use Databricks Utilities (dbutils) to list the files in each folder.\n",
    "# This confirms that the connection to Azure is working and shows you what data is available.\n",
    "dbutils.fs.ls(bronze_adls)\n",
    "dbutils.fs.ls(silver_adls)\n",
    "dbutils.fs.ls(gold_adls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14610a40-62cb-4b28-b7f9-4b7f0f4efa27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "start_date = date.today() - timedelta(1)\n",
    "end_date = date.today()\n",
    "start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f4e2fe-d27d-4e6f-9c12-b3eda935c449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Import specialized Spark functions.\n",
    "# 'when' is like an IF/ELSE statement for columns.\n",
    "# 'col' allows you to reference specific columns by name.\n",
    "# 'udf' (User Defined Function) lets you create your own custom Python logic for Spark.\n",
    "from pyspark.sql.functions import when, col, udf\n",
    "\n",
    "# 2. Tell Spark what kind of data your custom function will return.\n",
    "# In this case, we are returning text (the name of a city or country).\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 3. Import 'reverse_geocoder'. \n",
    "# This is a powerful library that takes Latitude/Longitude and tells you where that is on a map.\n",
    "# NOTE: This is a standard Python library, not a Spark one, which is why we need a UDF later.\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "# 4. Standard tools for date math.\n",
    "# We use these to find the specific files we want to process (e.g., \"yesterday's data\").\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67943e8f-544e-4522-ac25-55061ad931ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(silver_data).filter(col('time') > start_date)   \n",
    "df = df.limit(100) # added to speed up processings as during testing it was proving a bottleneck\n",
    "# The problem is caused by the Python UDF (reverse_geocoder) being a bottleneck due to its non-parallel nature and high computational cost per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6385224-5fa5-4306-bfca-748a92a0d5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_country_code(lat, lon):\n",
    "    \"\"\"\n",
    "    Retrieve the country code for a given latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    lat (float or str): Latitude of the location.\n",
    "    lon (float or str): Longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    str: Country code of the location, retrieved using the reverse geocoding API.\n",
    "\n",
    "    Example:\n",
    "    >>> get_country_details(48.8588443, 2.2943506)\n",
    "    'FR'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coordinates = (float(lat), float(lon))\n",
    "        result = rg.search(coordinates)[0].get('cc')\n",
    "        print(f\"Processed coordinates: {coordinates} -> {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {lat}, {lon} -> {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719945d0-2b99-4444-b125-a5c585682cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the get_country_code function as a Spark UDF that returns a string.\n",
    "get_country_code_udf = udf(get_country_code, StringType())\n",
    "\n",
    "# Add a new column \"country_code\" to the DataFrame by applying the UDF to the latitude and longitude columns.\n",
    "df_with_location = df.withColumn(\n",
    "    \"country_code\", get_country_code_udf(col(\"latitude\"), col(\"longitude\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec53596-0d2e-4797-85de-1d1bf1dc5f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# adding significance classification\n",
    "df_with_location_sig_class = \\\n",
    "                            df_with_location.\\\n",
    "                                withColumn('sig_class', \n",
    "                                            when(col(\"sig\") < 100, \"Low\").\\\n",
    "                                            when((col(\"sig\") >= 100) & (col(\"sig\") < 500), \"Moderate\").\\\n",
    "                                            otherwise(\"High\")\n",
    "                                            )\n",
    "df_with_location_sig_class.head()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389c13d2-85c2-4ae4-9936-409245869c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed DataFrame to the Silver container\n",
    "gold_output_path = f\"{gold_adls}earthquake_events_gold/\"\n",
    "\n",
    "# Append DataFrame to Silver container in Parquet format\n",
    "df_with_location_sig_class.write.mode('append').parquet(gold_output_path)\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

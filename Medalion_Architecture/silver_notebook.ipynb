{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11fc594-2e98-44df-9fed-7e0c557a5dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Reach out to the \"Bulletin Board\" (Task Values).\n",
    "# We tell it: \"Go find the task named 'bronze' and give me the value labeled 'bronze_output'.\"\n",
    "bronze_output = dbutils.jobs.taskValues.get(taskKey=\"Bronze\", key=\"bronze_output\")\n",
    "\n",
    "# 2. Extract the variables from that \"package\".\n",
    "\n",
    "# The .get(\"key\", \"\") is a safety feature: if the key is missing, it returns an empty string instead of crashing.\n",
    "start_date = bronze_output.get(\"start_date\", \"\")\n",
    "end_date = bronze_output.get(\"end_date\", \"\")\n",
    "bronze_adls = bronze_output.get(\"bronze_adls\", \"\")\n",
    "silver_adls = bronze_output.get(\"silver_adls\", \"\")\n",
    "gold_adls = bronze_output.get(\"gold_adls\", \"\")\n",
    "\n",
    "# 3. Print the results to verify we have the right \"keys\" to the storage account.\n",
    "print(f\"Start Date: {start_date}, Bronze ADLS: {bronze_adls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e816fa94-2f00-4b65-ae50-0daf745b6552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define the stages of the Medallion Architecture data pipeline\n",
    "tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "# 2. Automatically generate the storage URLs for each tier.\n",
    "# This creates a dictionary where each tier is a key, and the Azure storage path is the value.\n",
    "# The 'f' before the string allows us to plug the {tier} variable directly into the URL.\n",
    "adls_paths = {tier: f\"abfss://{tier}@earthquakestorageaccount.dfs.core.windows.net/\" for tier in tiers}\n",
    "adls_paths\n",
    "\n",
    "\n",
    "# 3. Pull the specific URLs out of our dictionary and save them to easy-to-use variables.\n",
    "# This is like taking a long address from a directory and writing it on a sticky note.\n",
    "bronze_adls = adls_paths[\"bronze\"]\n",
    "silver_adls = adls_paths[\"silver\"]\n",
    "gold_adls = adls_paths[\"gold\"] \n",
    "\n",
    "# 4. Use Databricks Utilities (dbutils) to list the files in each folder.\n",
    "# This confirms that the connection to Azure is working and shows you what data is available.\n",
    "dbutils.fs.ls(bronze_adls)\n",
    "dbutils.fs.ls(silver_adls)\n",
    "dbutils.fs.ls(gold_adls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977d0de3-d17d-4ba6-afdf-ea98b66cdf52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnull, when\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import date, timedelta\n",
    "     \n",
    "start_date = date.today() - timedelta(1)\n",
    "end_date = date.today()\n",
    "start_date, end_date\n",
    "\n",
    "# Load the JSON data into a Spark DataFrame\n",
    "df = spark.read.option(\"multiline\", \"true\").json(f\"{bronze_adls}{start_date}_earthquake_data.json\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86391916-94f5-4c82-b578-a5000c37846d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reshape earthquake data\n",
    "df = (\n",
    "    df\n",
    "    .select(\n",
    "        'id',\n",
    "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
    "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
    "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
    "        col('properties.title').alias('title'),\n",
    "        col('properties.place').alias('place_description'),\n",
    "        col('properties.sig').alias('sig'),\n",
    "        col('properties.mag').alias('mag'),\n",
    "        col('properties.magType').alias('magType'),\n",
    "        col('properties.time').alias('time'),\n",
    "        col('properties.updated').alias('updated')\n",
    "    )\n",
    ")\n",
    "\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78f0d05-ce47-49b6-be28-7cb7013a8682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Validate data: Check for missing or null values\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('longitude', when(isnull(col('longitude')), 0).otherwise(col('longitude')))\n",
    "    .withColumn('latitude', when(isnull(col('latitude')), 0).otherwise(col('latitude')))\n",
    "    .withColumn('time', when(isnull(col('time')), 0).otherwise(col('time')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f4e48a-ade7-4f3b-8582-5f160139f1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'time' and 'updated' to timestamp from Unix time\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('time', (col('time') / 1000).cast(TimestampType()))\n",
    "    .withColumn('updated', (col('updated') / 1000).cast(TimestampType()))\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d08e364-bf70-4bfd-89ab-7738759ac502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed DataFrame to the Silver container\n",
    "silver_output_path = f\"{silver_adls}earthquake_events_silver/\"\n",
    "\n",
    "# Append DataFrame to Silver container in Parquet format\n",
    "df.write.mode('append').parquet(silver_output_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac4c69c-a81a-4773-9ccd-ad13befd4e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key = \"silver_output\", value = silver_output_path) #we link the gold layer to the silver layer with this value  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
